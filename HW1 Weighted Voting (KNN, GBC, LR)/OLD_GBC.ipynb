{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble.gradient_boosting import GradientBoostingClassifier\n",
    "from sklearn.ensemble.gradient_boosting import MultinomialDeviance\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, y = load_iris(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyGBC:\n",
    "    def __init__(self, max_depth = 2, n_estimators=5,\n",
    "                 random_state=None, criterion='mse', learning_rate = 0.1):\n",
    "        self.max_depth = max_depth\n",
    "        self.n_estimators = n_estimators\n",
    "        self.random_state = random_state\n",
    "        self.criterion = criterion\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        X = X.astype(np.float32)\n",
    "        n_samples, self.n_features = X.shape\n",
    "        sample_weight = np.ones(n_samples, dtype=np.float32)\n",
    "        random_state = check_random_state(self.random_state)\n",
    "        \n",
    "        self.classes, y = np.unique(y, return_inverse=True)\n",
    "        self.n_classes = len(self.classes)\n",
    "        self.loss = MultinomialDeviance(self.n_classes)\n",
    "\n",
    "        self.init = self.loss.init_estimator()\n",
    "        self.estimators = np.empty((self.n_estimators, self.loss.K), dtype=np.object)\n",
    "        self.train_score_ = np.empty((self.n_estimators))\n",
    "\n",
    "        # fit initial model - FIXME make sample_weight optional\n",
    "        self.init.fit(X, y, sample_weight)\n",
    "\n",
    "        # init predictions\n",
    "        y_pred = self.init.predict(X)\n",
    "\n",
    "        # fit the boosting stages\n",
    "        n_stages = self._fit_stages(X, y, y_pred, sample_weight, random_state)\n",
    "        # change shape of arrays after fit (early-stopping or additional ests)\n",
    "        if n_stages != self.estimators.shape[0]:\n",
    "            self.estimators = self.estimators[:n_stages]\n",
    "            self.train_score_ = self.train_score_[:n_stages]\n",
    "            if hasattr(self, 'oob_improvement_'):\n",
    "                self.oob_improvement_ = self.oob_improvement_[:n_stages]\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def _fit_stages(self, X, y, y_pred, sample_weight, random_state):\n",
    "        n_samples = X.shape[0]\n",
    "        sample_mask = np.ones((n_samples, ), dtype=np.bool)\n",
    "\n",
    "        for i in range(self.n_estimators):\n",
    "            y_pred = self._fit_stage(i, X, y, y_pred, sample_weight,\n",
    "                                     sample_mask, random_state)\n",
    "\n",
    "            self.train_score_[i] = self.loss(y, y_pred)\n",
    "        return i + 1\n",
    "    \n",
    "    def _fit_stage(self, i, X, y, y_pred, sample_weight, sample_mask,  random_state):\n",
    "        \"\"\"Fit another stage of ``n_classes_`` trees to the boosting model. \"\"\"\n",
    "\n",
    "        original_y = y\n",
    "        \n",
    "        for k in range(self.n_classes):\n",
    "            y = np.array(original_y == k, dtype=np.float64)\n",
    "            residual = self.loss.negative_gradient(y, y_pred, k=k)\n",
    "\n",
    "            # induce regression tree on residuals\n",
    "            tree = DecisionTreeRegressor(\n",
    "                criterion=self.criterion,\n",
    "                max_depth=self.max_depth,\n",
    "                random_state=random_state)\n",
    "            \n",
    "            tree.fit(X, residual)\n",
    "            #self.loss.update_terminal_regions(tree.tree_, X, y, residual, y_pred,\n",
    "            #                             sample_weight, sample_mask,\n",
    "            #                             self.learning_rate, k=k)\n",
    "\n",
    "            self.estimators[i, k] = tree\n",
    "\n",
    "        return y_pred\n",
    "    \n",
    "    def predict(self, X):\n",
    "        score = self.decision_function(X)\n",
    "        decisions = self.loss._score_to_decision(score)\n",
    "        return self.classes.take(decisions, axis=0)\n",
    "    \n",
    "    def _decision_function(self, X):\n",
    "        score = self.init.predict(X).astype(np.float64)\n",
    "        for i in range(self.n_estimators):\n",
    "            for k in range(self.loss.K):\n",
    "                score[:,k] += self.estimators[i, k].predict(X)\n",
    "        return score\n",
    "    \n",
    "    def decision_function(self, X):\n",
    "        X = self.estimators[0, 0]._validate_X_predict(X, check_input=True)\n",
    "        score = self._decision_function(X)\n",
    "        if score.shape[1] == 1:\n",
    "            return score.ravel()\n",
    "        return score\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        return accuracy_score(y, self.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MYCLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = MyGBC(max_depth=2, n_estimators=5, random_state=42, criterion='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Xtrain' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-6c645178a2da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'Xtrain' is not defined"
     ]
    }
   ],
   "source": [
    "clf.fit(Xtrain, Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Xtest' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-3b4928e60cf5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'Xtest' is not defined"
     ]
    }
   ],
   "source": [
    "clf.score(Xtest, Ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SKLEARN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "skl_clf = GradientBoostingClassifier(max_depth=2, n_estimators=5, random_state=42,\n",
    "                                 criterion='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='mse', init=None, learning_rate=0.1,\n",
       "              loss='deviance', max_depth=2, max_features=None,\n",
       "              max_leaf_nodes=None, min_impurity_split=1e-07,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=5, presort='auto',\n",
       "              random_state=42, subsample=1.0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skl_clf.fit(Xtrain, Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94736842105263153"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skl_clf.score(Xtest, Ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ITER TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diff\tclf\t\tskl_clf\n",
      "0.0 0.947368421053 0.947368421053\n",
      "0.0 1.0 1.0\n",
      "0.0 0.921052631579 0.921052631579\n",
      "0.0 0.921052631579 0.921052631579\n",
      "0.0 0.894736842105 0.894736842105\n",
      "0.0263157894737 0.894736842105 0.921052631579\n",
      "0.0 0.973684210526 0.973684210526\n",
      "0.0 1.0 1.0\n",
      "0.0 0.947368421053 0.947368421053\n",
      "0.0 0.947368421053 0.947368421053\n",
      "0.0 0.947368421053 0.947368421053\n",
      "0.0526315789474 0.894736842105 0.947368421053\n",
      "0.0 0.894736842105 0.894736842105\n",
      "0.0263157894737 0.921052631579 0.947368421053\n",
      "0.0 0.921052631579 0.921052631579\n",
      "0.0 0.921052631579 0.921052631579\n",
      "0.0 0.868421052632 0.868421052632\n",
      "0.0526315789474 0.921052631579 0.973684210526\n",
      "0.0 0.921052631579 0.921052631579\n",
      "-0.0263157894737 1.0 0.973684210526\n",
      "0.0 0.921052631579 0.921052631579\n",
      "0.0263157894737 0.973684210526 1.0\n",
      "0.0 0.921052631579 0.921052631579\n",
      "0.0 0.868421052632 0.868421052632\n",
      "-0.0263157894737 0.921052631579 0.894736842105\n",
      "0.0263157894737 0.947368421053 0.973684210526\n",
      "0.0 0.947368421053 0.947368421053\n",
      "0.0 0.868421052632 0.868421052632\n",
      "0.0 0.973684210526 0.973684210526\n",
      "0.0 0.921052631579 0.921052631579\n",
      "0.0 0.921052631579 0.921052631579\n",
      "0.0 0.947368421053 0.947368421053\n",
      "0.0 0.921052631579 0.921052631579\n",
      "0.0263157894737 0.947368421053 0.973684210526\n",
      "0.0 0.947368421053 0.947368421053\n",
      "0.0 1.0 1.0\n",
      "0.0 0.921052631579 0.921052631579\n",
      "0.0 0.947368421053 0.947368421053\n",
      "0.0263157894737 0.921052631579 0.947368421053\n",
      "0.0 0.921052631579 0.921052631579\n",
      "0.0 0.973684210526 0.973684210526\n",
      "0.0263157894737 0.973684210526 1.0\n",
      "0.0 0.947368421053 0.947368421053\n",
      "0.0 1.0 1.0\n",
      "0.0 0.973684210526 0.973684210526\n",
      "0.0 0.921052631579 0.921052631579\n",
      "-0.0263157894737 0.947368421053 0.921052631579\n",
      "-0.0526315789474 0.921052631579 0.868421052632\n",
      "0.0 1.0 1.0\n",
      "0.0 0.947368421053 0.947368421053\n",
      "0.0526315789474 0.921052631579 0.973684210526\n",
      "0.0 0.973684210526 0.973684210526\n",
      "0.0 0.947368421053 0.947368421053\n",
      "0.0 0.921052631579 0.921052631579\n",
      "0.0 0.921052631579 0.921052631579\n",
      "0.0263157894737 0.973684210526 1.0\n",
      "0.0 0.947368421053 0.947368421053\n",
      "0.0 0.921052631579 0.921052631579\n",
      "0.0 0.894736842105 0.894736842105\n",
      "-0.0263157894737 0.973684210526 0.947368421053\n",
      "0.0 0.947368421053 0.947368421053\n",
      "0.0263157894737 0.921052631579 0.947368421053\n",
      "0.0 0.947368421053 0.947368421053\n",
      "0.0 0.894736842105 0.894736842105\n",
      "0.0 0.947368421053 0.947368421053\n",
      "0.0 0.894736842105 0.894736842105\n",
      "0.0 0.842105263158 0.842105263158\n",
      "0.0 0.947368421053 0.947368421053\n",
      "0.0 0.894736842105 0.894736842105\n",
      "0.0263157894737 0.947368421053 0.973684210526\n",
      "0.0 0.947368421053 0.947368421053\n",
      "0.0 0.921052631579 0.921052631579\n",
      "0.0 0.921052631579 0.921052631579\n",
      "0.0 0.894736842105 0.894736842105\n",
      "0.0 1.0 1.0\n",
      "0.0 0.947368421053 0.947368421053\n",
      "0.0263157894737 0.973684210526 1.0\n",
      "0.0 0.947368421053 0.947368421053\n",
      "-0.0263157894737 0.921052631579 0.894736842105\n",
      "0.0 1.0 1.0\n",
      "0.0789473684211 0.921052631579 1.0\n",
      "0.0 0.921052631579 0.921052631579\n",
      "0.0 0.973684210526 0.973684210526\n",
      "0.0 0.921052631579 0.921052631579\n",
      "0.0526315789474 0.921052631579 0.973684210526\n",
      "0.0 0.973684210526 0.973684210526\n",
      "0.0 0.973684210526 0.973684210526\n",
      "0.0 0.868421052632 0.868421052632\n",
      "0.0263157894737 0.947368421053 0.973684210526\n",
      "0.0 0.894736842105 0.894736842105\n",
      "0.0 0.921052631579 0.921052631579\n",
      "0.0 0.921052631579 0.921052631579\n",
      "0.0 0.921052631579 0.921052631579\n",
      "-0.0263157894737 1.0 0.973684210526\n",
      "0.0 0.894736842105 0.894736842105\n",
      "0.0 0.973684210526 0.973684210526\n",
      "0.0 0.921052631579 0.921052631579\n",
      "0.0 1.0 1.0\n",
      "0.0 0.973684210526 0.973684210526\n",
      "0.0 0.921052631579 0.921052631579\n"
     ]
    }
   ],
   "source": [
    "print 'diff\\tclf\\t\\tskl_clf'\n",
    "for i in xrange(100):\n",
    "    Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, y)\n",
    "    clf = MyGBC(max_depth=2, n_estimators=5, random_state=42, criterion='mse')\n",
    "    skl_clf = GradientBoostingClassifier(max_depth=2, n_estimators=5, random_state=42,\n",
    "                                 criterion='mse')\n",
    "    clf.fit(Xtrain, Ytrain)\n",
    "    skl_clf.fit(Xtrain, Ytrain)\n",
    "    sc1 = clf.score(Xtest, Ytest)\n",
    "    sc2 = skl_clf.score(Xtest, Ytest)\n",
    "    dif = sc2-sc1\n",
    "    print dif, sc1, sc2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# В некоторых случаях расхождение все-таки бывает, но это за рамками моего понимания"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BaseGradientBoosting(six.with_metaclass(ABCMeta, BaseEnsemble)):\n",
    "    \"\"\"Abstract base class for Gradient Boosting. \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def __init__(self, loss, learning_rate, n_estimators, criterion,\n",
    "                 min_samples_split, min_samples_leaf, min_weight_fraction_leaf,\n",
    "                 max_depth, min_impurity_decrease, min_impurity_split,\n",
    "                 init, subsample, max_features,\n",
    "                 random_state, alpha=0.9, verbose=0, max_leaf_nodes=None,\n",
    "                 warm_start=False, presort='auto',\n",
    "                 validation_fraction=0.1, n_iter_no_change=None,\n",
    "                 tol=1e-4):\n",
    "\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.loss = loss\n",
    "        self.criterion = criterion\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.min_weight_fraction_leaf = min_weight_fraction_leaf\n",
    "        self.subsample = subsample\n",
    "        self.max_features = max_features\n",
    "        self.max_depth = max_depth\n",
    "        self.min_impurity_decrease = min_impurity_decrease\n",
    "        self.min_impurity_split = min_impurity_split\n",
    "        self.init = init\n",
    "        self.random_state = random_state\n",
    "        self.alpha = alpha\n",
    "        self.verbose = verbose\n",
    "        self.max_leaf_nodes = max_leaf_nodes\n",
    "        self.warm_start = warm_start\n",
    "        self.presort = presort\n",
    "        self.validation_fraction = validation_fraction\n",
    "        self.n_iter_no_change = n_iter_no_change\n",
    "        self.tol = tol\n",
    "\n",
    "    def _fit_stage(self, i, X, y, y_pred, sample_weight, sample_mask,\n",
    "                   random_state, X_idx_sorted, X_csc=None, X_csr=None):\n",
    "        \"\"\"Fit another stage of ``n_classes_`` trees to the boosting model. \"\"\"\n",
    "\n",
    "        assert sample_mask.dtype == np.bool\n",
    "        loss = self.loss_\n",
    "        original_y = y\n",
    "\n",
    "        for k in range(loss.K):\n",
    "            if loss.is_multi_class:\n",
    "                y = np.array(original_y == k, dtype=np.float64)\n",
    "\n",
    "            residual = loss.negative_gradient(y, y_pred, k=k,\n",
    "                                              sample_weight=sample_weight)\n",
    "\n",
    "            # induce regression tree on residuals\n",
    "            tree = DecisionTreeRegressor(\n",
    "                criterion=self.criterion,\n",
    "                splitter='best',\n",
    "                max_depth=self.max_depth,\n",
    "                min_samples_split=self.min_samples_split,\n",
    "                min_samples_leaf=self.min_samples_leaf,\n",
    "                min_weight_fraction_leaf=self.min_weight_fraction_leaf,\n",
    "                min_impurity_decrease=self.min_impurity_decrease,\n",
    "                min_impurity_split=self.min_impurity_split,\n",
    "                max_features=self.max_features,\n",
    "                max_leaf_nodes=self.max_leaf_nodes,\n",
    "                random_state=random_state,\n",
    "                presort=self.presort)\n",
    "\n",
    "            if self.subsample < 1.0:\n",
    "                # no inplace multiplication!\n",
    "                sample_weight = sample_weight * sample_mask.astype(np.float64)\n",
    "\n",
    "            if X_csc is not None:\n",
    "                tree.fit(X_csc, residual, sample_weight=sample_weight,\n",
    "                         check_input=False, X_idx_sorted=X_idx_sorted)\n",
    "            else:\n",
    "                tree.fit(X, residual, sample_weight=sample_weight,\n",
    "                         check_input=False, X_idx_sorted=X_idx_sorted)\n",
    "\n",
    "            # update tree leaves\n",
    "            if X_csr is not None:\n",
    "                loss.update_terminal_regions(tree.tree_, X_csr, y, residual, y_pred,\n",
    "                                             sample_weight, sample_mask,\n",
    "                                             self.learning_rate, k=k)\n",
    "            else:\n",
    "                loss.update_terminal_regions(tree.tree_, X, y, residual, y_pred,\n",
    "                                             sample_weight, sample_mask,\n",
    "                                             self.learning_rate, k=k)\n",
    "\n",
    "            # add tree to ensemble\n",
    "            self.estimators_[i, k] = tree\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "    def _check_params(self):\n",
    "        if self.n_estimators <= 0:\n",
    "            raise ValueError(\"n_estimators must be greater than 0 but \"\n",
    "                             \"was %r\" % self.n_estimators)\n",
    "\n",
    "        if self.learning_rate <= 0.0:\n",
    "            raise ValueError(\"learning_rate must be greater than 0 but \"\n",
    "                             \"was %r\" % self.learning_rate)\n",
    "\n",
    "        if (self.loss not in self._SUPPORTED_LOSS\n",
    "                or self.loss not in LOSS_FUNCTIONS):\n",
    "            raise ValueError(\"Loss '{0:s}' not supported. \".format(self.loss))\n",
    "\n",
    "        if self.loss == 'deviance':\n",
    "            loss_class = (MultinomialDeviance\n",
    "                          if len(self.classes_) > 2\n",
    "                          else BinomialDeviance)\n",
    "        else:\n",
    "            loss_class = LOSS_FUNCTIONS[self.loss]\n",
    "\n",
    "        if self.loss in ('huber', 'quantile'):\n",
    "            self.loss_ = loss_class(self.n_classes_, self.alpha)\n",
    "        else:\n",
    "            self.loss_ = loss_class(self.n_classes_)\n",
    "\n",
    "        if not (0.0 < self.subsample <= 1.0):\n",
    "            raise ValueError(\"subsample must be in (0,1] but \"\n",
    "                             \"was %r\" % self.subsample)\n",
    "\n",
    "        if self.init is not None:\n",
    "            if isinstance(self.init, six.string_types):\n",
    "                if self.init not in INIT_ESTIMATORS:\n",
    "                    raise ValueError('init=\"%s\" is not supported' % self.init)\n",
    "            else:\n",
    "                if (not hasattr(self.init, 'fit')\n",
    "                        or not hasattr(self.init, 'predict')):\n",
    "                    raise ValueError(\"init=%r must be valid BaseEstimator \"\n",
    "                                     \"and support both fit and \"\n",
    "                                     \"predict\" % self.init)\n",
    "\n",
    "        if not (0.0 < self.alpha < 1.0):\n",
    "            raise ValueError(\"alpha must be in (0.0, 1.0) but \"\n",
    "                             \"was %r\" % self.alpha)\n",
    "\n",
    "        if isinstance(self.max_features, six.string_types):\n",
    "            if self.max_features == \"auto\":\n",
    "                # if is_classification\n",
    "                if self.n_classes_ > 1:\n",
    "                    max_features = max(1, int(np.sqrt(self.n_features_)))\n",
    "                else:\n",
    "                    # is regression\n",
    "                    max_features = self.n_features_\n",
    "            elif self.max_features == \"sqrt\":\n",
    "                max_features = max(1, int(np.sqrt(self.n_features_)))\n",
    "            elif self.max_features == \"log2\":\n",
    "                max_features = max(1, int(np.log2(self.n_features_)))\n",
    "            else:\n",
    "                raise ValueError(\"Invalid value for max_features: %r. \"\n",
    "                                 \"Allowed string values are 'auto', 'sqrt' \"\n",
    "                                 \"or 'log2'.\" % self.max_features)\n",
    "        elif self.max_features is None:\n",
    "            max_features = self.n_features_\n",
    "        elif isinstance(self.max_features, (numbers.Integral, np.integer)):\n",
    "            max_features = self.max_features\n",
    "        else:  # float\n",
    "            if 0. < self.max_features <= 1.:\n",
    "                max_features = max(int(self.max_features *\n",
    "                                       self.n_features_), 1)\n",
    "            else:\n",
    "                raise ValueError(\"max_features must be in (0, n_features]\")\n",
    "\n",
    "        self.max_features_ = max_features\n",
    "\n",
    "        if not isinstance(self.n_iter_no_change,\n",
    "                          (numbers.Integral, np.integer, type(None))):\n",
    "            raise ValueError(\"n_iter_no_change should either be None or an \"\n",
    "                             \"integer. %r was passed\"\n",
    "                             % self.n_iter_no_change)\n",
    "\n",
    "    def _init_state(self):\n",
    "        if self.init is None:\n",
    "            self.init_ = self.loss_.init_estimator()\n",
    "        elif isinstance(self.init, six.string_types):\n",
    "            self.init_ = INIT_ESTIMATORS[self.init]()\n",
    "        else:\n",
    "            self.init_ = self.init\n",
    "\n",
    "        self.estimators_ = np.empty((self.n_estimators, self.loss_.K),\n",
    "                                    dtype=np.object)\n",
    "        self.train_score_ = np.zeros((self.n_estimators,), dtype=np.float64)\n",
    "        # do oob?\n",
    "        if self.subsample < 1.0:\n",
    "            self.oob_improvement_ = np.zeros((self.n_estimators),\n",
    "                                             dtype=np.float64)\n",
    "\n",
    "    def _clear_state(self):\n",
    "        if hasattr(self, 'estimators_'):\n",
    "            self.estimators_ = np.empty((0, 0), dtype=np.object)\n",
    "        if hasattr(self, 'train_score_'):\n",
    "            del self.train_score_\n",
    "        if hasattr(self, 'oob_improvement_'):\n",
    "            del self.oob_improvement_\n",
    "        if hasattr(self, 'init_'):\n",
    "            del self.init_\n",
    "        if hasattr(self, '_rng'):\n",
    "            del self._rng\n",
    "\n",
    "    def _resize_state(self):\n",
    "        # self.n_estimators is the number of additional est to fit\n",
    "        total_n_estimators = self.n_estimators\n",
    "        if total_n_estimators < self.estimators_.shape[0]:\n",
    "            raise ValueError('resize with smaller n_estimators %d < %d' %\n",
    "                             (total_n_estimators, self.estimators_[0]))\n",
    "\n",
    "        self.estimators_.resize((total_n_estimators, self.loss_.K))\n",
    "        self.train_score_.resize(total_n_estimators)\n",
    "        if (self.subsample < 1 or hasattr(self, 'oob_improvement_')):\n",
    "            # if do oob resize arrays or create new if not available\n",
    "            if hasattr(self, 'oob_improvement_'):\n",
    "                self.oob_improvement_.resize(total_n_estimators)\n",
    "            else:\n",
    "                self.oob_improvement_ = np.zeros((total_n_estimators,),\n",
    "                                                 dtype=np.float64)\n",
    "\n",
    "    def _is_initialized(self):\n",
    "        return len(getattr(self, 'estimators_', [])) > 0\n",
    "\n",
    "    def _check_initialized(self):\n",
    "        check_is_fitted(self, 'estimators_')\n",
    "\n",
    "    @property\n",
    "    @deprecated(\"Attribute n_features was deprecated in version 0.19 and \"\n",
    "                \"will be removed in 0.21.\")\n",
    "    def n_features(self):\n",
    "        return self.n_features_\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None, monitor=None):\n",
    "        # if not warmstart - clear the estimator state\n",
    "        if not self.warm_start:\n",
    "            self._clear_state()\n",
    "\n",
    "        # Check input\n",
    "        X, y = check_X_y(X, y, accept_sparse=['csr', 'csc', 'coo'], dtype=DTYPE)\n",
    "        n_samples, self.n_features_ = X.shape\n",
    "        if sample_weight is None:\n",
    "            sample_weight = np.ones(n_samples, dtype=np.float32)\n",
    "        else:\n",
    "            sample_weight = column_or_1d(sample_weight, warn=True)\n",
    "\n",
    "        check_consistent_length(X, y, sample_weight)\n",
    "\n",
    "        y = self._validate_y(y)\n",
    "\n",
    "        if self.n_iter_no_change is not None:\n",
    "            X, X_val, y, y_val, sample_weight, sample_weight_val = (\n",
    "                train_test_split(X, y, sample_weight,\n",
    "                                 random_state=self.random_state,\n",
    "                                 test_size=self.validation_fraction))\n",
    "        else:\n",
    "            X_val = y_val = sample_weight_val = None\n",
    "\n",
    "        self._check_params()\n",
    "\n",
    "        if not self._is_initialized():\n",
    "            # init state\n",
    "            self._init_state()\n",
    "\n",
    "            # fit initial model - FIXME make sample_weight optional\n",
    "            self.init_.fit(X, y, sample_weight)\n",
    "\n",
    "            # init predictions\n",
    "            y_pred = self.init_.predict(X)\n",
    "            begin_at_stage = 0\n",
    "\n",
    "            # The rng state must be preserved if warm_start is True\n",
    "            self._rng = check_random_state(self.random_state)\n",
    "\n",
    "        else:\n",
    "            # add more estimators to fitted model\n",
    "            # invariant: warm_start = True\n",
    "            if self.n_estimators < self.estimators_.shape[0]:\n",
    "                raise ValueError('n_estimators=%d must be larger or equal to '\n",
    "                                 'estimators_.shape[0]=%d when '\n",
    "                                 'warm_start==True'\n",
    "                                 % (self.n_estimators,\n",
    "                                    self.estimators_.shape[0]))\n",
    "            begin_at_stage = self.estimators_.shape[0]\n",
    "            y_pred = self._decision_function(X)\n",
    "            self._resize_state()\n",
    "\n",
    "        X_idx_sorted = None\n",
    "        presort = self.presort\n",
    "        # Allow presort to be 'auto', which means True if the dataset is dense,\n",
    "        # otherwise it will be False.\n",
    "        if presort == 'auto' and issparse(X):\n",
    "            presort = False\n",
    "        elif presort == 'auto':\n",
    "            presort = True\n",
    "\n",
    "        if presort == True:\n",
    "            if issparse(X):\n",
    "                raise ValueError(\"Presorting is not supported for sparse matrices.\")\n",
    "            else:\n",
    "                X_idx_sorted = np.asfortranarray(np.argsort(X, axis=0),\n",
    "                                                 dtype=np.int32)\n",
    "\n",
    "        # fit the boosting stages\n",
    "        n_stages = self._fit_stages(X, y, y_pred, sample_weight, self._rng,\n",
    "                                    X_val, y_val, sample_weight_val,\n",
    "                                    begin_at_stage, monitor, X_idx_sorted)\n",
    "\n",
    "        # change shape of arrays after fit (early-stopping or additional ests)\n",
    "        if n_stages != self.estimators_.shape[0]:\n",
    "            self.estimators_ = self.estimators_[:n_stages]\n",
    "            self.train_score_ = self.train_score_[:n_stages]\n",
    "            if hasattr(self, 'oob_improvement_'):\n",
    "                self.oob_improvement_ = self.oob_improvement_[:n_stages]\n",
    "\n",
    "        self.n_estimators_ = n_stages\n",
    "        return self\n",
    "\n",
    "    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,\n",
    "                    X_val, y_val, sample_weight_val,\n",
    "                    begin_at_stage=0, monitor=None, X_idx_sorted=None):\n",
    "        n_samples = X.shape[0]\n",
    "        do_oob = self.subsample < 1.0\n",
    "        sample_mask = np.ones((n_samples, ), dtype=np.bool)\n",
    "        n_inbag = max(1, int(self.subsample * n_samples))\n",
    "        loss_ = self.loss_\n",
    "\n",
    "        # Set min_weight_leaf from min_weight_fraction_leaf\n",
    "        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:\n",
    "            min_weight_leaf = (self.min_weight_fraction_leaf *\n",
    "                               np.sum(sample_weight))\n",
    "        else:\n",
    "            min_weight_leaf = 0.\n",
    "\n",
    "        if self.verbose:\n",
    "            verbose_reporter = VerboseReporter(self.verbose)\n",
    "            verbose_reporter.init(self, begin_at_stage)\n",
    "\n",
    "        X_csc = csc_matrix(X) if issparse(X) else None\n",
    "        X_csr = csr_matrix(X) if issparse(X) else None\n",
    "\n",
    "        if self.n_iter_no_change is not None:\n",
    "            loss_history = np.ones(self.n_iter_no_change) * np.inf\n",
    "            # We create a generator to get the predictions for X_val after\n",
    "            # the addition of each successive stage\n",
    "            y_val_pred_iter = self._staged_decision_function(X_val)\n",
    "\n",
    "        # perform boosting iterations\n",
    "        i = begin_at_stage\n",
    "        for i in range(begin_at_stage, self.n_estimators):\n",
    "\n",
    "            # subsampling\n",
    "            if do_oob:\n",
    "                sample_mask = _random_sample_mask(n_samples, n_inbag,\n",
    "                                                  random_state)\n",
    "                # OOB score before adding this stage\n",
    "                old_oob_score = loss_(y[~sample_mask],\n",
    "                                      y_pred[~sample_mask],\n",
    "                                      sample_weight[~sample_mask])\n",
    "\n",
    "            # fit next stage of trees\n",
    "            y_pred = self._fit_stage(i, X, y, y_pred, sample_weight,\n",
    "                                     sample_mask, random_state, X_idx_sorted,\n",
    "                                     X_csc, X_csr)\n",
    "\n",
    "            # track deviance (= loss)\n",
    "            if do_oob:\n",
    "                self.train_score_[i] = loss_(y[sample_mask],\n",
    "                                             y_pred[sample_mask],\n",
    "                                             sample_weight[sample_mask])\n",
    "                self.oob_improvement_[i] = (\n",
    "                    old_oob_score - loss_(y[~sample_mask],\n",
    "                                          y_pred[~sample_mask],\n",
    "                                          sample_weight[~sample_mask]))\n",
    "            else:\n",
    "                # no need to fancy index w/ no subsampling\n",
    "                self.train_score_[i] = loss_(y, y_pred, sample_weight)\n",
    "\n",
    "            if self.verbose > 0:\n",
    "                verbose_reporter.update(i, self)\n",
    "\n",
    "            if monitor is not None:\n",
    "                early_stopping = monitor(i, self, locals())\n",
    "                if early_stopping:\n",
    "                    break\n",
    "\n",
    "            # We also provide an early stopping based on the score from\n",
    "            # validation set (X_val, y_val), if n_iter_no_change is set\n",
    "            if self.n_iter_no_change is not None:\n",
    "                # By calling next(y_val_pred_iter), we get the predictions\n",
    "                # for X_val after the addition of the current stage\n",
    "                validation_loss = loss_(y_val, next(y_val_pred_iter),\n",
    "                                        sample_weight_val)\n",
    "\n",
    "                # Require validation_score to be better (less) than at least\n",
    "                # one of the last n_iter_no_change evaluations\n",
    "                if np.any(validation_loss + self.tol < loss_history):\n",
    "                    loss_history[i % len(loss_history)] = validation_loss\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "        return i + 1\n",
    "\n",
    "    def _make_estimator(self, append=True):\n",
    "        # we don't need _make_estimator\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def _init_decision_function(self, X):\n",
    "        \"\"\"Check input and compute prediction of ``init``. \"\"\"\n",
    "        self._check_initialized()\n",
    "        X = self.estimators_[0, 0]._validate_X_predict(X, check_input=True)\n",
    "        if X.shape[1] != self.n_features_:\n",
    "            raise ValueError(\"X.shape[1] should be {0:d}, not {1:d}.\".format(\n",
    "                self.n_features_, X.shape[1]))\n",
    "        score = self.init_.predict(X).astype(np.float64)\n",
    "        return score\n",
    "\n",
    "    def _decision_function(self, X):\n",
    "        # for use in inner loop, not raveling the output in single-class case,\n",
    "        # not doing input validation.\n",
    "        score = self._init_decision_function(X)\n",
    "        predict_stages(self.estimators_, X, self.learning_rate, score)\n",
    "        return score\n",
    "\n",
    "\n",
    "    def _staged_decision_function(self, X):\n",
    "        X = check_array(X, dtype=DTYPE, order=\"C\",  accept_sparse='csr')\n",
    "        score = self._init_decision_function(X)\n",
    "        for i in range(self.estimators_.shape[0]):\n",
    "            predict_stage(self.estimators_, i, X, self.learning_rate, score)\n",
    "            yield score.copy()\n",
    "\n",
    "    @property\n",
    "    def feature_importances_(self):\n",
    "        self._check_initialized()\n",
    "\n",
    "        total_sum = np.zeros((self.n_features_, ), dtype=np.float64)\n",
    "        for stage in self.estimators_:\n",
    "            stage_sum = sum(tree.feature_importances_\n",
    "                            for tree in stage) / len(stage)\n",
    "            total_sum += stage_sum\n",
    "\n",
    "        importances = total_sum / len(self.estimators_)\n",
    "        return importances\n",
    "\n",
    "    def _validate_y(self, y):\n",
    "        self.n_classes_ = 1\n",
    "        if y.dtype.kind == 'O':\n",
    "            y = y.astype(np.float64)\n",
    "        # Default implementation\n",
    "        return y\n",
    "\n",
    "    def apply(self, X):\n",
    "        self._check_initialized()\n",
    "        X = self.estimators_[0, 0]._validate_X_predict(X, check_input=True)\n",
    "\n",
    "        # n_classes will be equal to 1 in the binary classification or the\n",
    "        # regression case.\n",
    "        n_estimators, n_classes = self.estimators_.shape\n",
    "        leaves = np.zeros((X.shape[0], n_estimators, n_classes))\n",
    "\n",
    "        for i in range(n_estimators):\n",
    "            for j in range(n_classes):\n",
    "                estimator = self.estimators_[i, j]\n",
    "                leaves[:, i, j] = estimator.apply(X, check_input=False)\n",
    "\n",
    "        return leaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MultinomialDeviance(ClassificationLossFunction):\n",
    "    is_multi_class = True\n",
    "\n",
    "    def __init__(self, n_classes):\n",
    "        if n_classes < 3:\n",
    "            raise ValueError(\"{0:s} requires more than 2 classes.\".format(\n",
    "                self.__class__.__name__))\n",
    "        super(MultinomialDeviance, self).__init__(n_classes)\n",
    "\n",
    "    def init_estimator(self):\n",
    "        return PriorProbabilityEstimator()\n",
    "\n",
    "    def __call__(self, y, pred, sample_weight=None):\n",
    "        # create one-hot label encoding\n",
    "        Y = np.zeros((y.shape[0], self.K), dtype=np.float64)\n",
    "        for k in range(self.K):\n",
    "            Y[:, k] = y == k\n",
    "\n",
    "        if sample_weight is None:\n",
    "            return np.sum(-1 * (Y * pred).sum(axis=1) +\n",
    "                          logsumexp(pred, axis=1))\n",
    "        else:\n",
    "            return np.sum(-1 * sample_weight * (Y * pred).sum(axis=1) +\n",
    "                          logsumexp(pred, axis=1))\n",
    "\n",
    "    def negative_gradient(self, y, pred, k=0, **kwargs):\n",
    "        \"\"\"Compute negative gradient for the ``k``-th class. \"\"\"\n",
    "        return y - np.nan_to_num(np.exp(pred[:, k] -\n",
    "                                        logsumexp(pred, axis=1)))\n",
    "\n",
    "    def _update_terminal_region(self, tree, terminal_regions, leaf, X, y,\n",
    "                                residual, pred, sample_weight):\n",
    "        \"\"\"Make a single Newton-Raphson step. \"\"\"\n",
    "        terminal_region = np.where(terminal_regions == leaf)[0]\n",
    "        residual = residual.take(terminal_region, axis=0)\n",
    "        y = y.take(terminal_region, axis=0)\n",
    "        sample_weight = sample_weight.take(terminal_region, axis=0)\n",
    "\n",
    "        numerator = np.sum(sample_weight * residual)\n",
    "        numerator *= (self.K - 1) / self.K\n",
    "\n",
    "        denominator = np.sum(sample_weight * (y - residual) *\n",
    "                             (1.0 - y + residual))\n",
    "\n",
    "        # prevents overflow and division by zero\n",
    "        if abs(denominator) < 1e-150:\n",
    "            tree.value[leaf, 0, 0] = 0.0\n",
    "        else:\n",
    "            tree.value[leaf, 0, 0] = numerator / denominator\n",
    "\n",
    "    def _score_to_proba(self, score):\n",
    "        return np.nan_to_num(\n",
    "            np.exp(score - (logsumexp(score, axis=1)[:, np.newaxis])))\n",
    "\n",
    "    def _score_to_decision(self, score):\n",
    "        proba = self._score_to_proba(score)\n",
    "        return np.argmax(proba, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GradientBoostingClassifier(BaseGradientBoosting, ClassifierMixin):\n",
    "\n",
    "    _SUPPORTED_LOSS = ('deviance', 'exponential')\n",
    "\n",
    "    def __init__(self, loss='deviance', learning_rate=0.1, n_estimators=100,\n",
    "                 subsample=1.0, criterion='friedman_mse', min_samples_split=2,\n",
    "                 min_samples_leaf=1, min_weight_fraction_leaf=0.,\n",
    "                 max_depth=3, min_impurity_decrease=0.,\n",
    "                 min_impurity_split=None, init=None,\n",
    "                 random_state=None, max_features=None, verbose=0,\n",
    "                 max_leaf_nodes=None, warm_start=False,\n",
    "                 presort='auto', validation_fraction=0.1,\n",
    "                 n_iter_no_change=None, tol=1e-4):\n",
    "\n",
    "        super(GradientBoostingClassifier, self).__init__(\n",
    "            loss=loss, learning_rate=learning_rate, n_estimators=n_estimators,\n",
    "            criterion=criterion, min_samples_split=min_samples_split,\n",
    "            min_samples_leaf=min_samples_leaf,\n",
    "            min_weight_fraction_leaf=min_weight_fraction_leaf,\n",
    "            max_depth=max_depth, init=init, subsample=subsample,\n",
    "            max_features=max_features,\n",
    "            random_state=random_state, verbose=verbose,\n",
    "            max_leaf_nodes=max_leaf_nodes,\n",
    "            min_impurity_decrease=min_impurity_decrease,\n",
    "            min_impurity_split=min_impurity_split,\n",
    "            warm_start=warm_start, presort=presort,\n",
    "            validation_fraction=validation_fraction,\n",
    "            n_iter_no_change=n_iter_no_change, tol=tol)\n",
    "\n",
    "    def _validate_y(self, y):\n",
    "        check_classification_targets(y)\n",
    "        self.classes_, y = np.unique(y, return_inverse=True)\n",
    "        self.n_classes_ = len(self.classes_)\n",
    "        return y\n",
    "\n",
    "    def decision_function(self, X):\n",
    "        X = check_array(X, dtype=DTYPE, order=\"C\",  accept_sparse='csr')\n",
    "        score = self._decision_function(X)\n",
    "        if score.shape[1] == 1:\n",
    "            return score.ravel()\n",
    "        return score\n",
    "\n",
    "    def staged_decision_function(self, X):\n",
    "        for dec in self._staged_decision_function(X):\n",
    "            # no yield from in Python2.X\n",
    "            yield dec\n",
    "\n",
    "    def predict(self, X):\n",
    "        score = self.decision_function(X)\n",
    "        decisions = self.loss_._score_to_decision(score)\n",
    "        return self.classes_.take(decisions, axis=0)\n",
    "\n",
    "    def staged_predict(self, X):\n",
    "        for score in self._staged_decision_function(X):\n",
    "            decisions = self.loss_._score_to_decision(score)\n",
    "            yield self.classes_.take(decisions, axis=0)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        score = self.decision_function(X)\n",
    "        try:\n",
    "            return self.loss_._score_to_proba(score)\n",
    "        except NotFittedError:\n",
    "            raise\n",
    "        except AttributeError:\n",
    "            raise AttributeError('loss=%r does not support predict_proba' %\n",
    "                                 self.loss)\n",
    "\n",
    "    def predict_log_proba(self, X):\n",
    "        proba = self.predict_proba(X)\n",
    "        return np.log(proba)\n",
    "\n",
    "    def staged_predict_proba(self, X):\n",
    "        try:\n",
    "            for score in self._staged_decision_function(X):\n",
    "                yield self.loss_._score_to_proba(score)\n",
    "        except NotFittedError:\n",
    "            raise\n",
    "        except AttributeError:\n",
    "            raise AttributeError('loss=%r does not support predict_proba' %\n",
    "                                 self.loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
